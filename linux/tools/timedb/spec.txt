
                    Time series DB specification

Why not RRD?
============

While RRD's idea seems good, its realization heavily sucks, really! The maximum
storage interval is 1s, the API is a nightmare (one needs to concatenate strings
that contain the value for storing) and it does not seem suited for
high-performance. The internal storage format has no concrete specification
and is quite intransparent. Reverse-engineering the rrdtool might rather be
a waste of time. Also, data compression is not always desired. Ideally, there
is a specification that is easy, simple and independent from the actual
implementation. Just like the Pcap file format. Then, this could also be used
for lightweight implementations on embedded devices (without having library
dependencies).

Architecture:
=============

File Header:
------------

struct timedb_hdr {
	uint32_t canary;
	uint8_t version_major;
	uint8_t version_minor;
	uint64_t interval;
	uint64_t start_tv_sec;
	uint64_t start_tv_usec;
	uint64_t entries;
	uint16_t cells_per_row;
} __packed;

Field description:

	- canary: is a field to identify the beginning of the file (default:0xdeadbeef)
	- version_major: as the name says, it's the major version number (default:1)
	- version_minor: as the name says, it's the minor version number (default:0)
	- interval: is the expected interval data arrives in us
	- start_tv_sec: start time, taken from a timeval
	- start_tv_usec: start time, taken from a timeval
	- entries: number of current entries
	- cells_per_row: number of columns per row

Data:
-----

Data is saved as array of 64-Bit floats. If cells_per_row is 3, then a data
block looks like:

struct my_block {
	float64_t cell[3];
} __packed;

This means that functionality is being left to the implementor, i.e. if he
includes min, max, avg values, if he smoothes data instead of raw values and
so on and so forth. New data is always appended at the end of the file, just
like log files. If a certain number of entries have been reached, then there
could exist a mechanism, that compresses the whole database and creates a new
one. However, portability across architectures is not guaranteed.
